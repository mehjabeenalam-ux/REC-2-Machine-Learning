{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKuZnOQh8EA2GLm0FefWE/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehjabeenalam-ux/REC-2-Machine-Learning/blob/main/EDA_Pandas_MiniProject_MissingData_DuplicateData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking Duplicate Values and Mossing Values"
      ],
      "metadata": {
        "id": "ApJOp2cF36se"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Pandas Module"
      ],
      "metadata": {
        "id": "Nu5Xba5j37kU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "tggkMu3r364G"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Data: Printing the entire dataset from the CSV File\n"
      ],
      "metadata": {
        "id": "mvafzCNa4BnH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv(\"data.csv\")  #Reads CSV File\n",
        "df = pd.read_csv(\"data.csv\")\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jk-Va5Do4BxG",
        "outputId": "2ec65464-1221-47f0-ca71-36686cb981fb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Date Category  Value   Product  Sales Region\n",
            "0    1/1/2023        A   28.0  Product1  754.0   East\n",
            "1    1/2/2023        B   39.0  Product3  110.0  North\n",
            "2    1/3/2023        C   32.0  Product2  398.0   East\n",
            "3    1/4/2023        B    8.0  Product1  522.0   East\n",
            "4    1/5/2023        B   26.0  Product3  869.0  North\n",
            "5    1/6/2023        B   54.0  Product3  192.0   West\n",
            "6    1/7/2023        A   16.0  Product1  936.0   East\n",
            "7    1/8/2023        C   89.0  Product1  488.0   West\n",
            "8    1/9/2023        C   37.0  Product3  772.0   West\n",
            "9   1/10/2023        A   22.0  Product2  834.0   West\n",
            "10  1/11/2023        B    7.0  Product1  842.0  North\n",
            "11  1/12/2023        B   60.0  Product2    NaN   West\n",
            "12  1/13/2023        A   70.0  Product3  628.0  South\n",
            "13  1/14/2023        A   69.0  Product1  423.0   East\n",
            "14  1/15/2023        A   47.0  Product2  893.0   West\n",
            "15  1/16/2023        C    NaN  Product1  895.0  North\n",
            "16  1/17/2023        C   93.0  Product2  511.0  South\n",
            "17  1/18/2023        C    NaN  Product1  108.0   West\n",
            "18  1/19/2023        A   31.0  Product2  578.0   West\n",
            "19  1/20/2023        A   59.0  Product1  736.0   East\n",
            "20  1/21/2023        C   82.0  Product3  606.0  South\n",
            "21  1/22/2023        C   37.0  Product2  992.0  South\n",
            "22  1/23/2023        B   62.0  Product3  942.0  North\n",
            "23  1/24/2023        C   92.0  Product2  342.0   West\n",
            "24  1/25/2023        A   24.0  Product2  458.0   East\n",
            "25  1/26/2023        C   95.0  Product1  584.0   West\n",
            "26  1/27/2023        C   71.0  Product2  619.0  North\n",
            "27  1/28/2023        C   56.0  Product2  224.0  North\n",
            "28  1/29/2023        B    NaN  Product3  617.0  North\n",
            "29  1/30/2023        C   51.0  Product2  737.0  South\n",
            "30  1/31/2023        B   50.0  Product3  735.0   West\n",
            "31   2/1/2023        A   17.0  Product2  189.0   West\n",
            "32   2/2/2023        B   63.0  Product3  338.0  South\n",
            "33   2/3/2023        C   27.0  Product3    NaN   East\n",
            "34   2/4/2023        C   70.0  Product3  669.0   West\n",
            "35   2/5/2023        B   60.0  Product2    NaN   West\n",
            "36   2/6/2023        C   36.0  Product3  177.0   East\n",
            "37   2/7/2023        C    2.0  Product1    NaN  North\n",
            "38   2/8/2023        C   94.0  Product1  408.0  South\n",
            "39   2/9/2023        A   62.0  Product1  155.0   West\n",
            "40  2/10/2023        B   15.0  Product1  578.0   East\n",
            "41  2/11/2023        C   97.0  Product1  256.0   East\n",
            "42  2/12/2023        A   93.0  Product3  164.0   West\n",
            "43  2/13/2023        A   43.0  Product3  949.0   East\n",
            "44  2/14/2023        A   96.0  Product3  830.0   East\n",
            "45  2/15/2023        B   99.0  Product2  599.0   West\n",
            "46  2/16/2023        B    6.0  Product1  938.0  South\n",
            "47  2/17/2023        B   69.0  Product3  143.0   West\n",
            "48  2/18/2023        C   65.0  Product3  182.0  North\n",
            "49  2/19/2023        C   11.0  Product3  708.0  North\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Basic Few Data Overview"
      ],
      "metadata": {
        "id": "UqWDbFxX4B9S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Printing Names of the Column Headings"
      ],
      "metadata": {
        "id": "LxCES-ad8l6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1YeIcDkL4CIU",
        "outputId": "e6e5589c-f096-4bc9-a884-efeaf61db057"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Date', 'Category', 'Value', 'Product', 'Sales', 'Region'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computing Number of Rows and Columns"
      ],
      "metadata": {
        "id": "AfPx0RYR4CTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NptNQVRI4Ccv",
        "outputId": "e7adfa89-18f4-4d01-c6a8-1cd40bc55b8b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Checking Missing Values"
      ],
      "metadata": {
        "id": "AwIg4QND6WsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count Missing Values in each column"
      ],
      "metadata": {
        "id": "CVc4qkfr6UZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.isnull().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Hx8AO3u6Uj_",
        "outputId": "771dbb11-81ce-49aa-86a2-ed9ab3c3d5d2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Date        0\n",
            "Category    0\n",
            "Value       3\n",
            "Product     0\n",
            "Sales       4\n",
            "Region      0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count Missing Values in each column"
      ],
      "metadata": {
        "id": "o90ZNumX6UtO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.isna().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1NiL6B46U1z",
        "outputId": "be58b96e-d07e-4f3d-da3b-d8f11d38e25e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Date        0\n",
            "Category    0\n",
            "Value       3\n",
            "Product     0\n",
            "Sales       4\n",
            "Region      0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Pandas, df.isna() and df.isnull() do exactly the same thing — they are identical functions that both return a DataFrame of the same shape as df with True where values are missing (NaN/None) and False otherwise.\n",
        "\n",
        "Short answer (one sentence):\n",
        "- There is no difference — isna() and isnull() are aliases (two different names for the same function) in Pandas, so df.isna().sum() and df.isnull().sum() always give identical results."
      ],
      "metadata": {
        "id": "rTnoBIsZ9MMN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display Rows with Missing Values"
      ],
      "metadata": {
        "id": "rrmmjDlt6U-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[df.isnull().any(axis=1)])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rq1E3rZd6VGv",
        "outputId": "5998a1eb-240c-4a08-94a7-adf608c28194"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Date Category  Value   Product  Sales Region\n",
            "11  1/12/2023        B   60.0  Product2    NaN   West\n",
            "15  1/16/2023        C    NaN  Product1  895.0  North\n",
            "17  1/18/2023        C    NaN  Product1  108.0   West\n",
            "28  1/29/2023        B    NaN  Product3  617.0  North\n",
            "33   2/3/2023        C   27.0  Product3    NaN   East\n",
            "35   2/5/2023        B   60.0  Product2    NaN   West\n",
            "37   2/7/2023        C    2.0  Product1    NaN  North\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Removes rows with Missing Values"
      ],
      "metadata": {
        "id": "wL0azvpW6VPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.dropna())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoXCD30c6VXT",
        "outputId": "8015293f-4975-4b94-d2fa-966d58bed59e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Date Category  Value   Product  Sales Region\n",
            "0    1/1/2023        A   28.0  Product1  754.0   East\n",
            "1    1/2/2023        B   39.0  Product3  110.0  North\n",
            "2    1/3/2023        C   32.0  Product2  398.0   East\n",
            "3    1/4/2023        B    8.0  Product1  522.0   East\n",
            "4    1/5/2023        B   26.0  Product3  869.0  North\n",
            "5    1/6/2023        B   54.0  Product3  192.0   West\n",
            "6    1/7/2023        A   16.0  Product1  936.0   East\n",
            "7    1/8/2023        C   89.0  Product1  488.0   West\n",
            "8    1/9/2023        C   37.0  Product3  772.0   West\n",
            "9   1/10/2023        A   22.0  Product2  834.0   West\n",
            "10  1/11/2023        B    7.0  Product1  842.0  North\n",
            "12  1/13/2023        A   70.0  Product3  628.0  South\n",
            "13  1/14/2023        A   69.0  Product1  423.0   East\n",
            "14  1/15/2023        A   47.0  Product2  893.0   West\n",
            "16  1/17/2023        C   93.0  Product2  511.0  South\n",
            "18  1/19/2023        A   31.0  Product2  578.0   West\n",
            "19  1/20/2023        A   59.0  Product1  736.0   East\n",
            "20  1/21/2023        C   82.0  Product3  606.0  South\n",
            "21  1/22/2023        C   37.0  Product2  992.0  South\n",
            "22  1/23/2023        B   62.0  Product3  942.0  North\n",
            "23  1/24/2023        C   92.0  Product2  342.0   West\n",
            "24  1/25/2023        A   24.0  Product2  458.0   East\n",
            "25  1/26/2023        C   95.0  Product1  584.0   West\n",
            "26  1/27/2023        C   71.0  Product2  619.0  North\n",
            "27  1/28/2023        C   56.0  Product2  224.0  North\n",
            "29  1/30/2023        C   51.0  Product2  737.0  South\n",
            "30  1/31/2023        B   50.0  Product3  735.0   West\n",
            "31   2/1/2023        A   17.0  Product2  189.0   West\n",
            "32   2/2/2023        B   63.0  Product3  338.0  South\n",
            "34   2/4/2023        C   70.0  Product3  669.0   West\n",
            "36   2/6/2023        C   36.0  Product3  177.0   East\n",
            "38   2/8/2023        C   94.0  Product1  408.0  South\n",
            "39   2/9/2023        A   62.0  Product1  155.0   West\n",
            "40  2/10/2023        B   15.0  Product1  578.0   East\n",
            "41  2/11/2023        C   97.0  Product1  256.0   East\n",
            "42  2/12/2023        A   93.0  Product3  164.0   West\n",
            "43  2/13/2023        A   43.0  Product3  949.0   East\n",
            "44  2/14/2023        A   96.0  Product3  830.0   East\n",
            "45  2/15/2023        B   99.0  Product2  599.0   West\n",
            "46  2/16/2023        B    6.0  Product1  938.0  South\n",
            "47  2/17/2023        B   69.0  Product3  143.0   West\n",
            "48  2/18/2023        C   65.0  Product3  182.0  North\n",
            "49  2/19/2023        C   11.0  Product3  708.0  North\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking Number of Rows and Columns after Rows with Missing Values"
      ],
      "metadata": {
        "id": "dXjkJ3WF7Joe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = df.dropna()\n",
        "print(df_clean.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84RSav8_7J3O",
        "outputId": "f3ddabfc-e6ae-4bd4-f1d6-1e8b2aed9bb0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(43, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Checking Duplicate Values"
      ],
      "metadata": {
        "id": "M322w_as5LWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checks Duplicate Values & returns a Boolean Series of Duplicate Values"
      ],
      "metadata": {
        "id": "f2qJORdo5ecm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.duplicated)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbQTJUaR5Lhu",
        "outputId": "12576e6d-2599-4abf-efac-5f134cf42bf5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method DataFrame.duplicated of          Date Category  Value   Product  Sales Region\n",
            "0    1/1/2023        A   28.0  Product1  754.0   East\n",
            "1    1/2/2023        B   39.0  Product3  110.0  North\n",
            "2    1/3/2023        C   32.0  Product2  398.0   East\n",
            "3    1/4/2023        B    8.0  Product1  522.0   East\n",
            "4    1/5/2023        B   26.0  Product3  869.0  North\n",
            "5    1/6/2023        B   54.0  Product3  192.0   West\n",
            "6    1/7/2023        A   16.0  Product1  936.0   East\n",
            "7    1/8/2023        C   89.0  Product1  488.0   West\n",
            "8    1/9/2023        C   37.0  Product3  772.0   West\n",
            "9   1/10/2023        A   22.0  Product2  834.0   West\n",
            "10  1/11/2023        B    7.0  Product1  842.0  North\n",
            "11  1/12/2023        B   60.0  Product2    NaN   West\n",
            "12  1/13/2023        A   70.0  Product3  628.0  South\n",
            "13  1/14/2023        A   69.0  Product1  423.0   East\n",
            "14  1/15/2023        A   47.0  Product2  893.0   West\n",
            "15  1/16/2023        C    NaN  Product1  895.0  North\n",
            "16  1/17/2023        C   93.0  Product2  511.0  South\n",
            "17  1/18/2023        C    NaN  Product1  108.0   West\n",
            "18  1/19/2023        A   31.0  Product2  578.0   West\n",
            "19  1/20/2023        A   59.0  Product1  736.0   East\n",
            "20  1/21/2023        C   82.0  Product3  606.0  South\n",
            "21  1/22/2023        C   37.0  Product2  992.0  South\n",
            "22  1/23/2023        B   62.0  Product3  942.0  North\n",
            "23  1/24/2023        C   92.0  Product2  342.0   West\n",
            "24  1/25/2023        A   24.0  Product2  458.0   East\n",
            "25  1/26/2023        C   95.0  Product1  584.0   West\n",
            "26  1/27/2023        C   71.0  Product2  619.0  North\n",
            "27  1/28/2023        C   56.0  Product2  224.0  North\n",
            "28  1/29/2023        B    NaN  Product3  617.0  North\n",
            "29  1/30/2023        C   51.0  Product2  737.0  South\n",
            "30  1/31/2023        B   50.0  Product3  735.0   West\n",
            "31   2/1/2023        A   17.0  Product2  189.0   West\n",
            "32   2/2/2023        B   63.0  Product3  338.0  South\n",
            "33   2/3/2023        C   27.0  Product3    NaN   East\n",
            "34   2/4/2023        C   70.0  Product3  669.0   West\n",
            "35   2/5/2023        B   60.0  Product2    NaN   West\n",
            "36   2/6/2023        C   36.0  Product3  177.0   East\n",
            "37   2/7/2023        C    2.0  Product1    NaN  North\n",
            "38   2/8/2023        C   94.0  Product1  408.0  South\n",
            "39   2/9/2023        A   62.0  Product1  155.0   West\n",
            "40  2/10/2023        B   15.0  Product1  578.0   East\n",
            "41  2/11/2023        C   97.0  Product1  256.0   East\n",
            "42  2/12/2023        A   93.0  Product3  164.0   West\n",
            "43  2/13/2023        A   43.0  Product3  949.0   East\n",
            "44  2/14/2023        A   96.0  Product3  830.0   East\n",
            "45  2/15/2023        B   99.0  Product2  599.0   West\n",
            "46  2/16/2023        B    6.0  Product1  938.0  South\n",
            "47  2/17/2023        B   69.0  Product3  143.0   West\n",
            "48  2/18/2023        C   65.0  Product3  182.0  North\n",
            "49  2/19/2023        C   11.0  Product3  708.0  North>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removes Duplicate Values"
      ],
      "metadata": {
        "id": "RHYyllB_5LsG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.drop_duplicates)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBcj3DSo5L36",
        "outputId": "5f9f705f-0cc6-48f4-8af4-cab74e48f628"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method DataFrame.drop_duplicates of          Date Category  Value   Product  Sales Region\n",
            "0    1/1/2023        A   28.0  Product1  754.0   East\n",
            "1    1/2/2023        B   39.0  Product3  110.0  North\n",
            "2    1/3/2023        C   32.0  Product2  398.0   East\n",
            "3    1/4/2023        B    8.0  Product1  522.0   East\n",
            "4    1/5/2023        B   26.0  Product3  869.0  North\n",
            "5    1/6/2023        B   54.0  Product3  192.0   West\n",
            "6    1/7/2023        A   16.0  Product1  936.0   East\n",
            "7    1/8/2023        C   89.0  Product1  488.0   West\n",
            "8    1/9/2023        C   37.0  Product3  772.0   West\n",
            "9   1/10/2023        A   22.0  Product2  834.0   West\n",
            "10  1/11/2023        B    7.0  Product1  842.0  North\n",
            "11  1/12/2023        B   60.0  Product2    NaN   West\n",
            "12  1/13/2023        A   70.0  Product3  628.0  South\n",
            "13  1/14/2023        A   69.0  Product1  423.0   East\n",
            "14  1/15/2023        A   47.0  Product2  893.0   West\n",
            "15  1/16/2023        C    NaN  Product1  895.0  North\n",
            "16  1/17/2023        C   93.0  Product2  511.0  South\n",
            "17  1/18/2023        C    NaN  Product1  108.0   West\n",
            "18  1/19/2023        A   31.0  Product2  578.0   West\n",
            "19  1/20/2023        A   59.0  Product1  736.0   East\n",
            "20  1/21/2023        C   82.0  Product3  606.0  South\n",
            "21  1/22/2023        C   37.0  Product2  992.0  South\n",
            "22  1/23/2023        B   62.0  Product3  942.0  North\n",
            "23  1/24/2023        C   92.0  Product2  342.0   West\n",
            "24  1/25/2023        A   24.0  Product2  458.0   East\n",
            "25  1/26/2023        C   95.0  Product1  584.0   West\n",
            "26  1/27/2023        C   71.0  Product2  619.0  North\n",
            "27  1/28/2023        C   56.0  Product2  224.0  North\n",
            "28  1/29/2023        B    NaN  Product3  617.0  North\n",
            "29  1/30/2023        C   51.0  Product2  737.0  South\n",
            "30  1/31/2023        B   50.0  Product3  735.0   West\n",
            "31   2/1/2023        A   17.0  Product2  189.0   West\n",
            "32   2/2/2023        B   63.0  Product3  338.0  South\n",
            "33   2/3/2023        C   27.0  Product3    NaN   East\n",
            "34   2/4/2023        C   70.0  Product3  669.0   West\n",
            "35   2/5/2023        B   60.0  Product2    NaN   West\n",
            "36   2/6/2023        C   36.0  Product3  177.0   East\n",
            "37   2/7/2023        C    2.0  Product1    NaN  North\n",
            "38   2/8/2023        C   94.0  Product1  408.0  South\n",
            "39   2/9/2023        A   62.0  Product1  155.0   West\n",
            "40  2/10/2023        B   15.0  Product1  578.0   East\n",
            "41  2/11/2023        C   97.0  Product1  256.0   East\n",
            "42  2/12/2023        A   93.0  Product3  164.0   West\n",
            "43  2/13/2023        A   43.0  Product3  949.0   East\n",
            "44  2/14/2023        A   96.0  Product3  830.0   East\n",
            "45  2/15/2023        B   99.0  Product2  599.0   West\n",
            "46  2/16/2023        B    6.0  Product1  938.0  South\n",
            "47  2/17/2023        B   69.0  Product3  143.0   West\n",
            "48  2/18/2023        C   65.0  Product3  182.0  North\n",
            "49  2/19/2023        C   11.0  Product3  708.0  North>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displays Duplicate Rows"
      ],
      "metadata": {
        "id": "9fnmyzfO6BO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[df.duplicated()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72LYwPv46Bcd",
        "outputId": "1e625a3f-8f70-4ad2-da54-efa47edd5d6e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Empty DataFrame\n",
            "Columns: [Date, Category, Value, Product, Sales, Region]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Counts Duplicate Rows"
      ],
      "metadata": {
        "id": "xE0ptBOq6Er5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.duplicated().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tc2nCTla6E2H",
        "outputId": "60fa486c-9925-4dae-a401-49062d556072"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary Statistics"
      ],
      "metadata": {
        "id": "5VloLN9b_GuF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mean of Numerical Columns\n",
        "# print(df.mean(axis=2))\n",
        "\n",
        "# df.mode(): Mode of Numerical Columns\n",
        "# df.median():  Median of Numerical Columns\n",
        "# df.stf(): Standard Deviation of Numerical Columns\n",
        "# df.var(): Variance of Numerical Columns\n",
        "\n",
        "# df.sum(): Sum of Numerical Columns\n",
        "# df.min(): Minimum Value of Numerical Columns\n",
        "# df.max(): Maximum Value of Numerical Columns\n",
        "# df.skew(): Skew of Numerical Columns\n",
        "# df.kurt(): Kurtosis of Numerical Columns\n",
        "# df.quantile(): Quantile of Numerical Columns\n",
        "# df.count():Count of Numerical Values per Columns\n",
        "# df.nunique(): Number of Unique Values per column"
      ],
      "metadata": {
        "id": "6ysymQ5z_G6e"
      },
      "execution_count": 31,
      "outputs": []
    }
  ]
}